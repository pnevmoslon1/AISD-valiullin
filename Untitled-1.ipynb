import pandas as pd
from nltk.tokenize import word_tokenize
import nltk
from nltk.stem.snowball import SnowballStemmer
import stop_words
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.pipeline import Pipeline

# Загрузка необходимых ресурсов NLTK
nltk.download('punkt')

# Инициализация стеммера и стоп-слов
stemmer = SnowballStemmer('russian')
stop_words_ru = stop_words.get_stop_words('russian')

# Функция для очистки текста
def clear_txt(txt):
    txt = txt.lower()
    txt = re.sub('[/+_!@#$A-Za-z0-9\n.,:()""«»;-]', ' ', txt)
    new_txt = ''
    for t in txt.split(' '):
        if len(t) > 0 and t not in stop_words_ru:  # Убираем стоп-слова
            new_txt = new_txt + stemmer.stem(t) + ' '
    return new_txt.strip()

# Загрузка данных
df_L = pd.read_csv('L.csv', sep=";", on_bad_lines='skip', encoding="Windows-1251")
df_C = pd.read_csv('C.csv', sep=";", on_bad_lines='skip', encoding="Windows-1251")

# Предобработка данных
df_L = df_L[['Desc', 'Group', 'Cat']].dropna()
df_C = df_C[['Desc']].dropna()  # Только Desc

# Очистка текстов
df_L['Desc'] = df_L['Desc'].apply(clear_txt)
df_C['Desc'] = df_C['Desc'].apply(clear_txt)

# Определение признаков и целевых переменных
X = df_L['Desc']
y = df_L[['Group', 'Cat']]  # Обе целевые переменные

# Создание пайплайна с MultiOutputClassifier
pipeline = Pipeline(steps=[
    ("tfidf", TfidfVectorizer()),
    ("classifier", MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))
])

# Обучение модели
pipeline.fit(X, y)

# Предсказание для df_C
predictions = pipeline.predict(df_C['Desc'])

# Добавление предсказаний в df_C
df_C['Predicted_Group'] = predictions[:, 0]  # Предсказания для Group
df_C['Predicted_Cat'] = predictions[:, 1]    # Предсказания для Cat

# Вывод результатов
print(df_C[['Desc', 'Predicted_Group', 'Predicted_Cat']])
